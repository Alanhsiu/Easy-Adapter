{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/virginiakm1988/Easy-Adapter/blob/main/example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter-efficient Fine-tuning in NLP\n",
        "\n",
        "This code demonstrates how to fine-tune a BERT model based on the Hugging Face Transformers library with [Easy-Adapter](https://github.com/virginiakm1988/Easy-Adapter). Adapters are a parameter-efficient way to fine-tune a pre-trained language model for a specific NLP task.\n",
        "This code demonstrates a practical example of using adapters in fine-tuning a BERT model. The code can be adapted to other pre-trained models and NLP tasks.\n",
        "\n",
        "For any suggestions or questions, please contact Zih-Ching Chen (virginia.chen2007@gmail.com)."
      ],
      "metadata": {
        "id": "Q6NKwvf6UXb-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URCCnICCUYbi"
      },
      "source": [
        "## Setup Instructions\n",
        "\n",
        "Before running the code, please follow these setup instructions:\n",
        "\n",
        "1. Install the necessary packages by running the following command: \n",
        "\n",
        "   ```\n",
        "   ! pip install transformers datasets\n",
        "   ! pip install loralib\n",
        "   ```\n",
        "\n",
        "2. Check that your system has a compatible GPU installed by running the following command in your terminal:\n",
        "\n",
        "   ```\n",
        "   nvidia-smi\n",
        "   ```\n",
        "\n",
        "\n",
        "Once you have completed these setup instructions, you are ready to run the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lynkbdevNmwV"
      },
      "outputs": [],
      "source": [
        "! pip install transformers datasets\n",
        "! pip install loralib\n",
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/virginiakm1988/Easy-Adapter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqTDdTQGY0S-",
        "outputId": "7f8b8c19-460b-43ad-9c0b-b3d23e5d8e17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Easy-Adapter' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Easy-Adapter/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB8zWTAzY3Ai",
        "outputId": "649ba53f-bbd3-49fb-f78f-751dbe08eac6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Easy-Adapter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVjQ1JTSUfNH"
      },
      "source": [
        "## Define custom adapter modules\n",
        "In [`adapters.py`](https://github.com/virginiakm1988/Easy-Adapter/blob/main/adapters.py), we implemented `Houlsby`, `ConvAdapters`, `AdapterBias`, and `LoRA`.\n",
        "1. Houlsby Adapter ([Parameter-Efficient Transfer Learning for NLP](https://http://proceedings.mlr.press/v97/houlsby19a.html))\n",
        "2. ConvAdapter ([CHAPTER: Exploiting Convolutional Neural Network Adapters for Self-supervised Speech Models](https://arxiv.org/abs/2212.01282))\n",
        "3. AdapterBias ([AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks](https://arxiv.org/abs/2205.00305))\n",
        "\n",
        "4. LoRA ([LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685))\n",
        "\n",
        "5. BitFit ([BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199)\n",
        "BitFit can be implemented through the following settings:\n",
        "```\n",
        "mark_only_adapter_as_trainable(model_bert,bias=\"all\")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfG0YD5ngHnQ"
      },
      "source": [
        "## Adding Adapter to a Hugging Face Model\n",
        "\n",
        "\n",
        "The code uses the `AutoModelForSequenceClassification` class to load a pre-trained BERT model (`bert-base-uncased` in this case). The code then sets the adapter type to \"houlsby\" and specifies the `lora_r` and `lora_alpha` parameters for the adapter.\n",
        "\n",
        "The code then modifies each layer of the BERT encoder to include the adapter module. The output layer is modified using the `adapted_bert_output` function from the `adapter_bert` module, and the attention layer is modified using the `AdaptedBertSelfAttention` class from the same module.\n",
        "\n",
        "Finally, the code freezes all parameters except for the adapter module by calling the `mark_only_adapter_as_trainable` function from the `utils` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec2TeCuA-YrR",
        "outputId": "bfd6cef8-9884-43cd-85fb-ea57cb21df00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from adapter_bert import adapted_bert_output, AdaptedBertSelfAttention\n",
        "from utils import mark_only_adapter_as_trainable\n",
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "BertLayerNorm = torch.nn.LayerNorm\n",
        "\n",
        "model_bert = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "model_bert.config.adapter = \"houlsby\"\n",
        "model_bert.config.lora_r = 8\n",
        "model_bert.config.lora_alpha = 8\n",
        "\n",
        "#add adapter module in a bert model\n",
        "for idx, layer in enumerate(model_bert.bert.encoder.layer):\n",
        "  #modify the output layer\n",
        "  model_bert.bert.encoder.layer[idx].output = adapted_bert_output(model_bert.bert.encoder.layer[idx].output, model_bert.config)\n",
        "  #modify the attention layer for adding lora\n",
        "  model_bert.bert.encoder.layer[idx].attention.self = AdaptedBertSelfAttention(model_bert.bert.encoder.layer[idx].attention.self, model_bert.config)\n",
        "\n",
        "#freeze parameters\n",
        "mark_only_adapter_as_trainable(model_bert)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFC1q8YzWfSs"
      },
      "source": [
        "## Loading datasets\n",
        "### Tokenizing an IMDb Dataset with BERT\n",
        "\n",
        "This code demonstrates a practical example of loading and tokenizing a dataset using the Hugging Face Datasets and Transformers libraries. The resulting tokenized datasets can be used for fine-tuning a pre-trained language model on the IMDb sentiment classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQukWUmBWdY_"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "raw_datasets = load_dataset(\"imdb\")\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "def tokenize_fu \n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "full_train_dataset = tokenized_datasets[\"train\"]\n",
        "full_eval_dataset = tokenized_datasets[\"test\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsyZ19r6WkBi"
      },
      "source": [
        "## Training a BERT Model on IMDb Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "i4oPxeZrWebn",
        "outputId": "85b3865e-4914-4110-ca32-2570b963a618"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [750/750 01:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.696800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=750, training_loss=0.6981643676757813, metrics={'train_runtime': 62.0895, 'train_samples_per_second': 48.317, 'train_steps_per_second': 12.079, 'total_flos': 794857457664000.0, 'train_loss': 0.6981643676757813, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "training_args = TrainingArguments(output_dir = \"test-trainer\",per_device_train_batch_size = 4)\n",
        "trainer = Trainer(\n",
        "    model=model_bert, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset\n",
        ")\n",
        "trainer.train() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5vqaOMRgYqC"
      },
      "source": [
        "## Saving Adapter State Dictionary in a BERT Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcJ3ygwogbwg"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"./result\"\n",
        "torch.save(adapter_state_dict(model_bert), checkpoint_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}